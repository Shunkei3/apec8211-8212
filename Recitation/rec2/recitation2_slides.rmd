---
title: "APEC8211: Recitation 2"
author: 
  - "Shunkei Kakimoto" 
header-includes:
   - \usepackage{mathtools}
   - \usepackage{color}
   - \usepackage{amsmath}
output:
  xaringan::moon_reader:
    self_contained: true
    css: xaringan-themer.css 
    lib_dir: libs
    nature: 
      highlightStyle: github
      highlightLines: true
---
class: middle

```{r, child = '../setup.rmd', cache = FALSE}
```

```{r, include = F, cache = FALSE}
# here::i_am("GitControlled/Recitation/1_Introduction/recitation1_slides.rmd")
```


```{r, include = F, cache = FALSE}
library(data.table)
library(ggplot2)
library(dplyr)
library(gganimate)
library(gifski)
library(gganimate)
```

```{r, include = F, eval=F, cache = FALSE}
httpgd::hgd()
httpgd::hgd_browse()
```

# Outline

Review some concepts related to random variables

<!-- # main  -->
[1. CDF, PDF, PMF (Quick review)](#dist) 
  + [Exercise problem 1](#ex1)
  + [Exercise problem 2 (optional)](#ex2)

<!-- # To explain Jensen's inequality -->
[2. Mean and variance and covariance(Quick review)](#mean)
  + [Exercise problem 3](#ex3)
  + [Exercise problems 4 (optional)](#ex4)

[3. Introduction of Monte Calro Simulation](#monte)
[Supplement: Jensen's inequality (Quick review)](#jensen)

---
class: inverse, center, middle
name: dist

# CDF, PDF, and PMF

<html><div style='float:left'></div><hr color='#EB811B' size=1px width=796px></html>

---
.content-box-red[**Distribution function**]
+ Cumulative distribution function (CDF)
+ Definition: <span style="color:red">The CDF of a random variable $X$ is $F(x) = Pr[X \leq x]$</span>
+ **Verbally**: CDF $F(x)$ tells us the probability of the event that random variable $X$ is <span style="color:red">less</span> than a value $x$. 

.left5[
```{r, echo=F}
# Define sequence of x-values  
x <-  seq(-4, 4, .01)
# CDF plot for a continuous variable
cdf_x <- pnorm(x)
# plot normal CDF
plot(
  x, cdf_x, type="l",
  xlab = "X",
  ylab = "Probability",
  main = "Continuous distribution function"
)
```
]
.right5[
```{r, dpi=36, echo=F}
# https://stackoverflow.com/questions/66266703/draw-discrete-cdf-in-r
x <- -1:5
prob <- ppois(q = x, lambda = 3)
n <- length(x)

plot(x = NA, y = NA, pch = NA, 
     xlim = c(-1, max(x)), 
     ylim = c(0, 1),
     xlab = "X",
     ylab = "Probability",
     main = "Discrete distribution function"
)
points(x = x[-1], y = prob[-1], pch=19)
points(x = x[-1], y = prob[-n], pch=1)
for(i in 2:(n-1))
  points(x=x[i+0:1], y=prob[c(i,i)], type="l")
points(x=c(-2,x[2]), y=prob[c(1,1)], type="l")
points(x=c(x[n],6), y=prob[c(n,n)], type="l")
```
]

---
<!-- PDF and PMF (1) -->

.content-box-red[**Probability mass function (Discrete random variables)**]
+ **Definition**: $\color{red}{\pi(x) = Pr[X = x]}$
+ **Verbally**: The probability that $X$ equals the value $x$

<br>

.content-box-red[**Probability density function (Continuous random variables)**] 
+ **Definition**:  $\color{red}{f(x) = \frac{d}{dx}F(x)} \quad ( = \displaystyle \lim_{h\to\infty} \frac{F(x+h)-F(x)}{h})$
+ **Verbally**: Density function is defined as a very small change in the CDF (or the probability of the random variable falling within a particular range of values according to [wikipedia](https://en.wikipedia.org/wiki/Probability_density_function)). 

---
<!-- PDF and PMF (2) -->

.content-box-red[**Probability mass function (Discrete random variables)**]
+ **Definition**: $\color{red}{\pi(x) = Pr[X = x]}$
+ **Verbally**: The probability that $X$ equals the value $x$

<br>

.content-box-red[**Probability density function (Continuous random variables)**] 
+ **Definition**:  $\color{red}{f(x) = \frac{d}{dx}F(x)} \quad ( = \displaystyle \lim_{h\to\infty} \frac{F(x+h)-F(x)}{h})$
+ **Verbally**: Density function is a very small change in the CDF (or the probability of the random variable falling within a particular range of values according to [wikipedia](https://en.wikipedia.org/wiki/Probability_density_function)). 


<br>
.content-box-red[**Theorem 2.3: Properties of a PDF**] 

A function f(x) is a density function **if and only if** 

$$\begin{cases}
f(x) \ge 0 \text{ for all } x \\
\int_{-\infty}^\infty f(x)\,dx = 1
\end{cases}$$

+ You can use this condition to check whether a function is valid density function is or not!
<!-- if you are asked to show that a function f(x) is a valid density function, check whether f(x) satisfies these properties or not. -->


---
class: middle

.content-box-green[**Relationship between CDF and PDF**]
+ **From CDF to PDF**: $f(x) = \frac{d}{dx}F(x)$ </br> (by definition of PDF)

+ **From PDF to CDF**: $F(x) = Pr(X \leq x) = \int_{-\infty}^x f(t) dt$ </br> (as shown below)


```{r, child = 'prepareResults.rmd', cache=T}
```

---
name: ex1

# Exercise 1
.content-box-green[**Final Exam: 2021: Problem 1**] 

Define $\Phi(z)$ as the CDF of a standard normal random variable and $\phi(z)$ as its density function. 

(a) Write $Pr(Z \leq b)$ using $\Phi()$.

(b) Write $Pr(Z \leq b)$ as an integral. 

(c) Write $Pr(a \leq Z \leq b)$ using $\Phi()$.

(d) Write $Pr(a \leq Z \leq b)$ as an integral.

---
# Exercise 2
.content-box-green[**PSE Exercise 2.1**] 

Let $X \sim U[0,1]$. Find the PDF of random variable $Y=X^2$. 

---
class: inverse, center, middle
name: mean

# Mean and Variance and Covariance

<html><div style='float:left'></div><hr color='#EB811B' size=1px width=796px></html>
---

.content-box-red[**Mean and Variance**]

**Definition** 2.18, 2.19: 

+ The mean of $X$ is<span style='color:red'> $E[X]$</span>
+ The variance of $X$ is <span style='color:red'> $Var[X]=E[(X-E[X])^2]$</span> $= E[X^2] - (E[X])^2$


```{r,echo=F, cache=T}
# /*===========================================*/
#'=  Mean of X =
# /*===========================================*/
# /*===== Data generation =====*/
# --- Set the range of X --- #
x_left <- 2
x_right <- 10
x_center <- (x_left + x_right)/2

# --- Creating data --- #
x <- seq(x_left, x_right, length = 1000)
y <- dnorm(x, mean = x_center, sd = 1)
plot_data <- data.table(x = x, y = y)

# /*===== Visualization =====*/
plot_mean_X <- 
  ggplot(data = plot_data) +
    geom_line(aes(x = x, y = y), color = "black")+
    geom_vline(xintercept=x_center, color="red") +
    labs(y="Density", title="Density of X ~ N(6,1)") +
    annotate("text", x = 7, y = 0.01,
      label = "Mean E[X]=6",
      size = 3, color="red") +
    theme_bw()+
    theme(plot.title = element_text(hjust = 0.5))
```

```{r, echo=F, cache=T}
# /*===========================================*/
#'= Variance =
# /*===========================================*/
# /*===== Data generation =====*/
# --- a list of the values of Var[X] --- #
ls_var <- c(1,4,9)
# --- Create an object to save results--- #
ls_res <- vector(mode='list', length=length(ls_var))
# --- Range of X --- #
x_left <- 2
x_right <- 10
x_center <- (x_left + x_right)/2

# --- Generate data by the value in ls_var --- #
for (i in seq(1:length(ls_var))){
  var <- ls_var[i]
  x <- seq(x_left, x_right, length = 1000)
  y <- dnorm(x, mean = x_center, sd = sqrt(var))
  ls_res[[i]] <- data.table(var = var, x = x, y = y)
}
res_total <- rbindlist(ls_res)

# /*===== Visualization =====*/
plot_var_X <- 
  ggplot(data = res_total) +
  geom_line(aes(x = x, y = y, color = interaction(var)))+
  labs(y = "Density", title = expression("Density of X ~ N(6," ~ sigma^2 ~ ") with various " ~ sigma^2)) +
  guides(color = guide_legend(title= expression(sigma^2 ~ "=")))+
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5))
```


.left5[
```{r, dpi=36, echo=F, out.width = "100%", cache=T}
plot_mean_X
```
  ]

.right5[
```{r, dpi=36, echo=F, out.width = "100%", cache=T}
plot_var_X
```
  ]

---

.content-box-red[**Covariance**]

**Definition**
$$
\color{red}{Cov(X, Y) = E[(X-E[X])((Y-E[Y]))]} = E[XY] - E[X][Y]
$$

**Verbally**: Covariance measure the joint variability of two random variables. 

+ .content-box-green[Visualization]

```{r, dpi=36,, echo=F, fig.dim = c(12, 3)}
# /*===== Data generation =====*/
ls_a <- c(-0.8, -0.5, 0, 0.5, 0.8)
n = 1000
ls_result <- list()

set.seed(123)
x <- rnorm(n, 3, sd=2)
z <- rnorm(n, 3, sd=2)

for (i in seq(1:length(ls_a))){  
  a <- ls_a[i]
  y <- a*x + abs(1-abs(a))*z
  ls_result[[i]] <- data.table(a = a, x = x, y = y)
}

results <- 
  rbindlist(ls_result) %>%
  .[,type := paste0("Cov(X,Y)=",round(cov(x,y), digits = 1)), by=a]

# /*===== Visualization =====*/
ggplot(data = results)+
  geom_point(aes(x = x, y = y), size = 0.5)+
  facet_wrap(~ factor(type, unique(results$type)), scale = "free", nrow = 1)+
  labs(x = "X", y = "Y", title = "Plots of random variables (X,Y) with different sizes of covariances")+
  theme_bw()+
  theme(
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank(),
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank(),
    plot.title = element_text(hjust = 0.5)
  )
```


---

.content-box-red[**Covariance**]

**Definition**
$$
\color{red}{Cov(X, Y) = E[(X-E[X])((Y-E[Y]))]} = E[XY] - E[X][Y]
$$

**Verbally**: Covariance measure the joint variability of two random variables. 

+ .content-box-green[Visualization]

```{r, dpi=36,, echo=F, fig.dim = c(12, 3)}
# /*===== Data generation =====*/
ls_a <- c(-0.8, -0.5, 0, 0.5, 0.8)
n = 1000
ls_result <- list()

set.seed(123)
x <- rnorm(n, 3, sd=2)
z <- rnorm(n, 3, sd=2)

for (i in seq(1:length(ls_a))){  
  a <- ls_a[i]
  y <- a*x + abs(1-abs(a))*z
  ls_result[[i]] <- data.table(a = a, x = x, y = y)
}

results <- 
  rbindlist(ls_result) %>%
  .[,type := paste0("Cov(X,Y)=",round(cov(x,y), digits = 1)), by=a]

# /*===== Visualization =====*/
ggplot(data = results)+
  geom_point(aes(x = x, y = y), size = 0.5)+
  facet_wrap(~ factor(type, unique(results$type)), scale = "free", nrow = 1)+
  labs(x = "X", y = "Y", title = "Plots of random variables (X,Y) with different sizes of covariances")+
  theme_bw()+
  theme(
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank(),
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank(),
    plot.title = element_text(hjust = 0.5)
  )
```

.content-box-red[**Correlation**]

**Definition**
$$
\color{red}{Corr(X, Y) = \frac{Cov(X,Y)}{\sqrt{Var[X] Var[Y]}}}
$$

---
## Play around with data (optional)
.content-box-green[**Goal**]
+ Know some basic R functions (e.g., mean, variance, etc.)
+ See covariance is influenced by the change in scale but correlation is not. 

.small-code[
```{r, echo=T, eval=F}
# === Data === #
data(airquality)
# ?airquality

#/*--------------------------------*/
#' ## Basic functions of R
#/*--------------------------------*/
# --- histogram of Temperature (Temp) --- #
hist(airquality$Temp)
# frequency table can be obtained by running table(airquality$Temp)

# --- Mean of Temp (degrees F)--- #
mean(airquality$Temp)

# --- Variance of Temp --- #
var(airquality$Temp)
# sd(airquality$Temp) for standard deviation
# --- summary statistics of Temp --- #
summary(airquality$Temp)

#/*------------------------------------------*/
#' ## Relationship between Wind and Temp
#/*------------------------------------------*/
plot(airquality$Wind, airquality$Temp)

# === Covariance === #
cov(airquality$Wind, airquality$Temp)

# What happens if you change the unit of wind from mph to kmph (1mph=1.6kmph)
cov(airquality$Wind*1.6, airquality$Temp)

# === Correlation === #
cor(airquality$Wind, airquality$Temp)

# What happens if you change the unit of wind from mph to kmph (1mph=1.6kmph)
cor(airquality$Wind*1.6, airquality$Temp)
```
]

---


## E[ ], Var[ ] as operators

.content-box-red[**Expectation: E[ ]**]

<span style="color:red"> $E[ \,]$ is a linear operator (Linearity of expectation)</span>

For any constants $a$ and $b$, 

$$E[a+bX] = a + bE[X]$$
$$E[aX+bY] = aE[X] + bE[Y]$$




But, if $g(\cdot)$ function is nonlinear function, then $E[g(X_1, X_2, ... X_k)] \neq g(E[\cdot])$ unless 

+ Example) $E[X_1]$





---

## E[ ], Var[ ] as operators

.content-box-red[**Expectation: E[ ]**]

<span style="color:red"> $E[ \,]$ is a linear operator (Linearity of expectation)</span>

For any constants $a$ and $b$, 

$$E[a+bX] = a + bE[X]$$

.content-box-red[**Variance: Var[ ]**]

$Var[ \,]$ is <span style="color:red">not</span> a linear operator

$$
Var[a+bX] = b^2E[X]
$$

because 
$$\begin{align*}
Var[a+bX]
  &= E[(a+bX - E[a+bX])^2] \\ 
  &= E[(a+bX - a-bE[X])^2] \\ 
  &= E[(b(X - E[X]))^2] \\ 
  &= E[b^2(X - E[X])^2] \\ 
  &= b^2 (X - E[X])^2 \\ 
  &= b^2 Var[X]
\end{align*}$$


---
# Exercise 4

.content-box-green[**Lecture note 2, p14**] 

Prove these for continuous $(X,Y)$ with finite variances.

(a). If $E[X]=0$ or $E[Y]=0$, $Cov(X,Y)=E[XY]$.

(b). If $X \perp\!\!\!\perp Y$, $corr(X,Y)=0$.

(c). If $E[X] = E[Y] = 0$, $Var[X+Y] = Var[X] + Var[Y] + 2Cov(X,Y)$ (Note: Also true if the expectations are non-zero).

(d). If $X$ and $Y$ are uncorrelated, $Var[X+Y] = Var[X] + Var[Y]$.

---
# Exercise 3

.content-box-green[**Final Exam: 2021: Problem 3**] 

The chi-squared distribution with $k$ degrees of freedom, denoted $\chi^2(k)$, is the distribution of $\sum_{i=1}^k Z^2_{i}$ and the $Z_i$ are independent $(Z_i \perp\!\!\!\perp Z_j)$. *You do not need to work with the CDF or density of a $\chi^2$ distribution to answer this question!*.

<br>

(a) Show that if $X$ is distributed $\chi^2(k)$ then $E[X]=k$.

<br>

(b) More work with expectation Let $K=Z^2_{1} + Z^2_{2}$, where $Z_j \sim N(0,1)$. Then $K \sim \chi^2(2)$. Another fact is that if $Z \sim N(0,1)$, then $E[Z^4]=3$. Use that fact to show that $Var[K]=4$. [Hint: $E[Z_j^4]$ is closely related to $Var[Z_j^2]$.]


---

class: inverse, center, middle
name: intro


# Monte Calro Simulation

<html><div style='float:left'></div><hr color='#EB811B' size=1px width=796px></html>

---
class: middle
.content-box-red[**Monte Carlo Simulation**]
+ A way to test econometric theories or statistical procedures in realistic setting via simulation. 

---

class: middle

.content-box-green[**Example: Binomial distribution (PSE 3.4)**]

The binomial disrandom variables equals the outcome of $n$ independent Bernoulli trials. If you flip a coin $n$ times, the number of heads has a binomial distribution. 

<br>

Theoretically, the binomial random variable has a binomial distribution with 
$$\begin{align*}
E[X] &= np \\
Var[X] &= np(1-p)
\end{align*}$$

<br>

Can we confim this with Monte Calro simulation?

---
class: middle

class: middle

.content-box-green[**Example: Binomial distribution (PSE 3.4)**]

Suppose that we flip a coin $n=9$ times, and count the number of heads (i.e. $X$). The coin is not fair, $p=Pr[heads]=\frac{1}{3}$.

Theoretically, 

+ $E[X]=np = 9 \times \frac{1}{3} = 3$
+ $Var[X]=np(1-p) = 9 \times \frac{1}{3} (1 - \frac{1}{3}) = 2$

---
class: middle

## Monte Carlo Simulaton: Steps
1. specify the data generaing process
2. generate data based on the data generating process
3. get an outcome you are insterested in based on the generated data
4. repeat step 2 and 3 many many times 

5. compare your estimates with the true parameter

---

Theoretically, $E[X]= 3$ and $Var[X] = 2$. 

.medium-code[
```{r, echo=T, eval=T}
set.seed(1234)

# --- Step1: Speficify the data generating process --- #
p <- 1/3 
n <- 9 # the number of trials

# --- Step2: generate data  --- #
seq_x <- sample(c(1,0), size=9, prob = c(p, 1-p), replace=TRUE)

seq_x
# --- Step3: get an outcome you are insterested in --- #
sum(seq_x)
```
]

---

# Step 4: For loop

```{r,  out.width = "80%"}
# --- Step4: repeat step 2 and 3 many many times  --- #
B <- 1000 # the number of iterations

# create a storage that stores outcomes
storage <- numeric(B)

for(i in 1:B){
  seq_x <- sample(c(1,0), size=9, prob = c(p, 1-p), replace=TRUE)
  storage[i] <- sum(seq_x)
}

mean(storage)

hist(storage)
```



```{r, echo=F, eval=F}
# r: 3
# w: 3
# b: 6
library(data.table)# w: 3

n <- 1000
d = 0.1
B = 100


set.seed(1234)


res <- numeric(B)


for(j in 1:B){
  data <- 
  data.table(
    id = 1:n ,
    drug = sample(c("Yes","No"), size=n, prob = c(d, 1-d), replace=TRUE),
    response = rep("na", n)
    )

  for(i in 1:n){
  x <- sample(c("r", "w", "b"), size=1, prob = c(3/12, 3/12, 6/12))
  # x="b"
  if(x=="r"){
    data[id==i, response:= "Yes"]
  }
  if(x=="w"){
    data[id==i, response:= "No"]
  }
  if(x=="b"){
    data[id==i, response:= ifelse(drug=="Yes", "Yes", "No")]
  }
  }
  pr_yes_hat <- nrow(data[response=="Yes"])/n

  res[[j]] <- 2*(pr_yes_hat - 1/4)

}


mean(res)

n=10000

data <- 
  data.table(
    id = 1:n ,
    drug = sample(c("Yes","No"), size=n, prob = c(d, 1-d), replace=TRUE),
    response = rep("na", n)
    )

nrow(data[drug=="Yes",])/n


  for(i in 1:n){
  x <- sample(c("r", "w", "b"), size=1, prob = c(3/12, 3/12, 6/12))
  # x="b"
  if(x=="r"){
    data[id==i, response:= "Yes"]
  }
  if(x=="w"){
    data[id==i, response:= "No"]
  }
  if(x=="b"){
    data[id==i, response:= ifelse(drug=="Yes", "Yes", "No")]
  }
  }

  pr_yes_hat <- nrow(data[response=="Yes",])/n

  2*(pr_yes_hat - 1/4)
```




---
class: inverse, center, middle
name: jensen

# Jensen's inequality

<html><div style='float:left'></div><hr color='#EB811B' size=1px width=796px></html>
---


## Motivation

Linearity of expectation cannot be used when the function inside $E[\,]$ is a nonlinear function.

.content-box-green[**Example:**] 
+ If $g(X)$ is a linear function (e.g., $g(x)=ax+b$)
  * $E[g(X)]=g(E[X]])$
+ If $g(X)$ is a nonlinear function (e.g., $g(x)=x^2$)
  * $E[g(X)] \neq g(E[X])$

---

(This is not the proof)

In the previous slide, we saw $Var[X]=E[(X-E[X])^2]=E[X^2] - (E[X])^2$. Because, $Var[X] \ge 0$ (by the way, $Var[X]=0$ if and only if $X$ is degenerate). 


$$E[X^2] - (E[X])^2 \ge 0$$
<p style="text-align: center;">or</p>
$$(E[X])^2 \leq E[X^2]$$
Define $g(x)=x^2$. Then it is written as
$$g(E[X]) \leq E[g(X)]$$

Generally, 
$$\begin{align*}
g(E[X]) \leq E[g(X)] \quad &\text{if } g(x) \text{ is a convex function} \\
E[g(X)] \leq g(E[X]) \quad &\text{if } g(x) \text{ is a concave function}
\end{align*}$$

---
.content-box-green[**Visualization**]

.panelset[ 

.panel[.panel-name[Example 1 : g(x) is convex]
.left5[

Suppose that $g(x)=x^2$.

.small-code[
```{r, echo=T}
set.seed(356)

# Create a sequence of X from a uniformal distribution
x <- runif(1000, 0, 10)

# /*===== Convex case: g(X)=X^2 =====*/
y <- x^2

figure_ex1 <-
  ggplot()+
  geom_point(aes(x = x, y = y))+
  # --- E[X] --- #
  geom_vline(xintercept = mean(x), color = "red", linetype = "dashed")+
  annotate("text", x = mean(x)+1, y = 0.01,
    label = paste0("E[X]=", round(mean(x), 1)), size = 3, color = "red") +
  # --- Add horizontal line for --- #
  geom_hline(yintercept = mean(y), color="blue", linetype = "dashed")+
  annotate("text", x = 1, y = mean(y)+5,
    label = paste0("E[g(X)]=", round(mean(y), 1)), size = 3, color = "blue") +
  # --- Add horizontal line for g(E[X]) --- #
  geom_hline(yintercept = mean(x)^2, color="darkgreen", linetype = "dashed")+
  annotate("text", x = 1, y = mean(x)^2-5,
    label = paste0("g(E(X))=", round(mean(x)^2, 1)), size = 3, color = "darkgreen") +
  theme_bw()
```
  ]
  ]
.right5[
```{r, echo=F, out.width = "100%"}
figure_ex1
```

$$\color{darkgreen}{g(E[X])} \leq \color{blue}{E[g(X)]}$$
  ]
]

.panel[.panel-name[Example 2: g(x) is concave]
.left5[

Suppose that $g(x)=\sqrt{x}$.

.small-code[
```{r, echo=T, out.width = "50%"}
# /*===== Convex case: g(X)=X^(1/2) =====*/
y <- x^(1/2)

figure_ex2 <-
  ggplot()+
  geom_point(aes(x = x, y = y))+
  # --- E[X] --- #
  geom_vline(xintercept = mean(x), color = "red", linetype = "dashed")+
  annotate("text", x = mean(x)+0.8, y = 0.01,
    label = paste0("E[X]=", round(mean(x), 1)), size = 3, color = "red") +
  # --- E[g(X)] --- #
  geom_hline(yintercept = mean(y), color = "blue", linetype = "dashed")+
  annotate("text", x = 1, y = mean(y)-0.2,
    label = paste0("E[g(X)]=", round(mean(y), 2)), size = 3, color = "blue") +
  # --- g(E[X]) --- #
  geom_hline(yintercept = mean(x)^(1/2), color = "darkgreen", linetype = "dashed")+
  annotate("text", x = 1, y = mean(x)^(1/2)+0.2,
    label = paste0("g(E(X))=", round(mean(x)^(1/2), 2)), size = 3, color = "darkgreen") +
  theme_bw()
```
  ]
  ]
.right5[
```{r, echo=F, out.width = "100%"}
figure_ex2
```
$$\color{blue}{E[g(X)]} \leq \color{darkgreen}{g(E[X])}$$
  ]
]
]


---





---
Question:


The problem sets in the assignment covers the materials from the previous week. 


**Question: Should I cover the material we learned from this week,  the previous week or the mix of the two?**

<br>

**option (1): lab covers the material from this week**

.content-box-green[**pro:**] The material in the lab is the review of the class on this week and it will be helpful for the next assignment. 

.content-box-red[**con:**] You may not have time to review the textbook or class-notes, so the lab would be boring. 

<br>

**option (2): lab covers the material from the previous week**
  
.content-box-green[**pro:**] You have enough time to review the textbook and class-note, so it is more likely to understand the lab

.content-box-red[**con:**]





