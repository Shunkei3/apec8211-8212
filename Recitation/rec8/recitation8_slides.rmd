---
title: "APEC8211: Recitation 8"
author: 
  - "Shunkei Kakimoto" 
header-includes:
   - \usepackage{mathtools}
   - \usepackage{color}
   - \usepackage{amsmath}
output:
  xaringan::moon_reader:
    self_contained: true
    css: ../xaringan-themer.css 
    lib_dir: libs
    nature: 
      highlightStyle: github
      highlightLines: true
---
class: middle

```{r, child = '../setup.rmd', cache = FALSE}
```

```{r xaringan-scribble, echo=FALSE}
xaringanExtra::use_scribble()
```


```{r setup, include=FALSE, cache = FALSE}
options(htmltools.dir.version = FALSE)

# /*===== Reference =====*/
suppressMessages(library(RefManageR))

BibOptions(
  check.entries = FALSE,
  bib.style = "authoryear",
  style = "markdown",
  hyperlink = FALSE,
  dashed = TRUE,
  max.names = 2,
  longnamesfirst = FALSE
)
bib <- ReadBib("cite.bib")
```

```{r, include = F, cache = FALSE}
library(data.table)
library(ggplot2)
library(dplyr)
library(gganimate)
library(gifski)
library(gganimate)
# library(learnr)
```

```{r, include = F, eval=F, cache = FALSE}
httpgd::hgd()
httpgd::hgd_browse()
```

```{r xaringan-tachyons, echo=FALSE}
xaringanExtra::use_tachyons()
```


.content-box-green[**A Useful tip:**]

hitting letter "o" key will give you a panel view of the slides

---
class: middle

# Outline
1. Review Delta Method
2. [Exercise 1](#ex1)
3. [Exercise 2](#ex2)

---
class: middle

Consider a plug-in estimator $\hat{\beta}=h(\hat{\theta})$. We want to estimate the variacne of the plug-in estimator.

$$Var[\hat{\beta}]?$$

???

+ To understand the power of the delta method, let's think about a couple of examples. 
+ Suppose that we want to know the variance of a plug-in estimator which is a function of $\hat{\theta}$. 

---
class: middle

.content-box-green[**Case 1: If h is a linear function:**]

<b>Example </b>: $h(u) = a + b \cdot u$

Then 
$$Var[\hat{\beta}]=Var[h(\hat{\theta})]=Var[a + b \cdot \hat{\theta}]= b^2 Var[\hat{\theta}]$$

--
class: middle

.content-box-green[**Case 1: If h is a linear function:**]

<b>Example </b>: $h(u) = a + b \cdot u$

Then 
$$Var[\hat{\beta}]=Var[h(\hat{\theta})]=Var[a + b \cdot \hat{\theta}]= b^2 Var[\hat{\theta}]$$

<span style="color:blue">This can be done easily.</span>

---

class: middle

.content-box-green[**Case 2: If h is a nonlinear function:**]

<b>Example 1</b>

An odds ratio, $\hat{\beta} = h(\hat{\theta}) = \frac{\hat{\theta}_1}{\hat{\theta}_2}$

<br>

<b>Example 2</b> (Assignment 5 Question 3)

The ME estimator for a probit, $\hat{\Delta}_{D}=\Phi(\overline{X}^{\prime} \hat{\beta} + \hat{\delta}) - \Phi(\overline{X}^{\prime}\hat{\beta})$


???
+ Okay, next, let's think about the case wherte h function is a nonlinear function.
+ For example, our estimator of interest can be an odds ratio which is a ratio of the two estimator. 
  *  *The odds ratio is defined as the ratio of the odds of A in the presence of B and the odds of A in the absence of B*

+ Or, it can be a marginal effect estimator for a probit like Question 3 in assignment 5.  
+ By the way, what $\Phi()$ means?
  * Because the logit model assumes that the error is 


---

class: middle

.content-box-green[**Case 2: If h is a nonlinear function:**]

<b>Example 1</b>

An odds ratio $\hat{\beta} = h(\hat{\theta}) = \frac{\hat{\theta}_1}{\hat{\theta}_2}$. 

<br>

<b>Example 2</b> (Assignment 5 Question 3)

The ME estimator for a probit, $\hat{\Delta}_{D}=\Phi(\overline{X}^{\prime} \hat{\beta} + \hat{\delta}) - \Phi(\overline{X}^{\prime}\hat{\beta})$


<br>

<span style="color:blue">Direct derivation of $Var[\hat{\beta}]$ and $Var[\hat{\Delta}_{D}]$ are difficult.</span> 

&rarr; Estimate the asymptotic variance of $\hat{\beta}$ and $\hat{\Delta}_{D}$ using <span style="color:red">the delta method</span>. 

???
+ Then, the direct derivation of variance formula is so complicated. 
+ In this case, we can rely on the delta method and estimate the asymptotic variance of the plug-in estimator. 


---

class: middle 

<!-- .content-box-green[**Motivation**]

We would like to know about the distribution of the plug-in estimator $\hat{\beta}=h(\hat{\theta})$ using what we know about the moment estimator $\hat{\theta}$.  -->

Consider the plug-in estimator $\hat{\beta}=h(\hat{\theta})$. 

.bg-washed-green.b--dark-green.ba.bw2.br3.shadow-2.ph2.mt2[
**<span style="color:red">Delta method</span>**

If a function $h(u)$ is continuously differentiable in a neighborhood of $\theta$ and if the standardized sequence $\sqrt{n}(\hat{\theta}-\theta) \xrightarrow{d} N(0, \mathbf{V})$, then as $n \rightarrow \infty$

\begin{align*}
\sqrt{n}(h(\hat{\theta})-h(\theta)) \xrightarrow{d} N(0, \mathbf{h^{\prime}Vh})
\end{align*}

where, $\mathbf{h}= \nabla h(\theta)$ (the gradient of $h$, evaluated at $\mathbf{\theta}$)

]

<!-- **Verbally:** Differentiable functions of asymptotically normal random estimators are asymptotically normal.  -->

???
+ Okay, let's do a quick review of the delta method. 
+ The pug-in estimator $\hat{\beta}$ is a function of another estimator which is $\hat{\theta}$. If $\hat{\theta}$ is asymptotically normal, then the $\hat{\beta}$ is also asymptotically normal. 
+ The most important point in the delta method is that, it tells us the asymptotica variance of the 
+ The big picture here is that differentiable functions of asymptotically normal random estimators are asymptotically normal.

+ $h^{\prime}Vh$ is the asymptoric variance of the $\hat{\beta}$. 

---
class: middle 

.content-box-green[**Key takeaways**]

(1) The Delta method says that continuously differentiable functions of asymptotically normal random estimators are asymptotically normal.


(2) <span style="color:red">With the delta method, we can approximate the asymptotic variance of a nonlinear estimator</span> that otherwise would be very difficult to derive and estimate. (Very cool!)

&rarr; Once you get the asymptotic variance formula by the delta method, you can derive the plug-in estimator for the variance. 


---
class: middle 

# Exercise 1

For univariate $X$ with finite variance, $\sqrt{n}(\overline{X}_n - \mu) \xrightarrow{d} N(0, \sigma^2)$. 

(1) Find the asymptotic variances for the standardized sequences of the folling plug-in estimators using the delta method, respectively.  

+  $exp(\overline{X}_n)$
+  $log(\overline{X}_n)$

(2) Choose either the plug-in estimator of $exp(\overline{X}_n)$ and $log(\overline{X}_n)$. Let's run Monte Carlo simulations to see how much the the asymptotic variance you just derived correctly estimates the true variance. (You might want to use large sample size).

---
class: middle 

# Exercise 2

Suppose 

$$ln y_t = \beta_0 + \beta_1 ln y_{t-1} + \beta_2 ln x_t + \varepsilon_t$$

The long-run, or equibrium, elasticity of $y$ with respect to $x$ is $\theta=\frac{\beta_2}{1-\beta_1}$, estimated as $\theta=\frac{\hat{\beta}_2}{1-\hat{\beta}_1}$. Use the delta method to derive the asymptotic variance of $\hat{\theta}$.

---
class: middle

The Delta Method is also used to derive the asymptotic distribution of some test statistics. 

.content-box-green[**Question:**] 

What is the asymptotic distribution of t-statistic and the Wald statistic?


$$T(\theta)=\frac{\hat{\theta}-\theta}{s(\hat{\theta})} \xrightarrow{d} ?$$

$$W(\theta)=(\hat{\theta}-\theta)^{\prime}\hat{\mathbf{V}}^{-1}_{\hat{\theta}}(\hat{\theta}-\theta) \xrightarrow{d} ?$$


???

+ We learned that for hypothetical testing, we need a sampling distribution of test statistics. But we never know the sampling distribution of the test statistic, so we need a asymptotic distribution of the test statistics
  * another approach that does not depend on the asymptotic distribution is bootstrapping. 

---
class: middle

The Delta Method is also used to derive the asymptotic distribution of some test statistics. 

<br>

.content-box-green[**Question:**] 

What is the asymptotic distribution of t-statistic and the Wald statistic?

See E 7.12
$$T(\theta)=\frac{\hat{\theta}-\theta}{s(\hat{\theta})} \xrightarrow{d} N(0,1)$$

See E 7.16
$$W(\theta)=(\hat{\theta}-\theta)^{\prime}\hat{\mathbf{V}}^{-1}_{\hat{\theta}}(\hat{\theta}-\theta) \xrightarrow{d} \chi^2_{k}$$

???
+ About t-stat, because $s(\hat{V}_\theta)=\sqrt{\hat{V}_\theta}$, $V_\theta= n V_{\theta}$, and $\sqrt{n}(\hat{\theta}-\theta) \xrightarrow{d} N(0, \V_{\theta})$
