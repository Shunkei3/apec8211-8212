---
title: "APEC8212: Recitation 10"
author: 
  - "Shunkei Kakimoto" 
header-includes:
   - \usepackage{mathtools}
   - \usepackage{color}
   - \usepackage{amsmath}
output:
  xaringan::moon_reader:
    self_contained: true
    css: ../xaringan-themer.css 
    lib_dir: libs
    nature: 
      highlightStyle: github
      highlightLines: true
---
class: middle

```{r, child = '../setup.rmd', cache = FALSE}
```

```{r xaringan-scribble, echo=FALSE}
xaringanExtra::use_scribble()
```


```{r setup, include=FALSE, cache = FALSE, eval=F}
options(htmltools.dir.version = FALSE)

# /*===== Reference =====*/
suppressMessages(library(RefManageR))

BibOptions(
  check.entries = FALSE,
  bib.style = "authoryear",
  style = "markdown",
  hyperlink = FALSE,
  dashed = TRUE,
  max.names = 2,
  longnamesfirst = FALSE
)
bib <- ReadBib("cite.bib")
```

```{r, include = F, cache = FALSE}
library(data.table)
library(ggplot2)
library(dplyr)
library(gganimate)
library(gifski)
library(gganimate)
# library(learnr)
```

```{r, include = F, eval=F, cache = FALSE}
httpgd::hgd()
httpgd::hgd_browse()
```

```{r xaringan-tachyons, echo=FALSE}
xaringanExtra::use_tachyons()
```


.content-box-green[**A Useful tip:**]

hitting letter "o" key will give you a panel view of the slides

---
class: middle 

.content-box-red[**A heads-up**]

I modified the slides in Recitation 9 uploaded on Canvas [[here](https://shunkei3.github.io/apec8211-8212/Recitation/rec9/recitation9_slides.html)]. 

+ Specifically, the slides about the example problem for the calculation of $s(\hat{\beta}_1, \hat{\beta}_2)$ and the derivation of $\mathbf{R}$ vector. 
+ I modified the PDF version of it as well. 

???
+ Just a heads-up, 

---
class: middle

# Outline
1.[Review Resampling methods](#Resampling)

+ [Exercise problem](#ex)

2. [Code review for assignment 7](#code)

---
class: inverse, center, middle
name: Resampling

# Resampling methods

<html><div style='float:left'></div><hr color='#EB811B' size=1px width=796px></html>

---
class: middle

## Overview:Resampling methods

.content-box-green[**1. Jackknife resampling**]
  * use the distribution of the leave-on-out estimator
  * it is useful when an explicit asymptotic variance formula is not available (E 10.3)
  * but "the advent of the bootstrap has caused practitioners to lose interest in the jackknife" (Kennedy 2008)

<br>

.content-box-green[**2. Bootstrapping**]

<br>

.content-box-green[**3. Randomization inference**]
  * mostly used in RCT designs 
    - but this approach also has been getting used in the context of regression discontinuity designs and regression kink designs (A good summary can be found in [He√ü and Simon (2017)](https://journals.sagepub.com/doi/pdf/10.1177/1536867X1701700306))

???
+ Resampling is to create new samples from the observed sample. 
+ We learned three different types of resampling methods. 
+ The first one is Jackknife resampling methods. 
  * In this method, we resample the data by omitting each of the observations in turn and apply OLS to that resampled data. In other words, we repeat leave-one-out regression. So, if we have n observations, we need to run n regressions.
  * The distribution of the coefficient estimates derived from leave-one-out regressions are called a Jacknife distribution. 
  * Jacknife estimator was used when an asymptotic formula for covariance matrix estimator is not available. But, it has been replaced by bootstrap methods. 

+ The second one is bootstrap method, and we spent lots of time doing this. 
+ The third is randomization inference. 


---
class: middle

## Overview: Bootstrap methods

.content-box-green[**Resampling principle:**]

+ Bootstrap DGP should replicate the feature of population DGP (e.g., heteroskedasticity, clustering, time dependence).

<br>

.content-box-green[**Asymptotic refinement:**]

+ Bootstrap an <span style="color:blue">asymptotically pivotal statistic</span> gives a better better approximation oto the sampling distribution than the asymptotic distribution.
  * Examples of asymptotically pivotal statistic are t-statistic $(t \xrightarrow{d} N(0,1))$, Wald statistic $(W\xrightarrow{d}\chi^2_q \quad(q \text{ means } q \text{ restrictions}))$.

???
+ About bootstrap methods, there are two things that we should be aware of. 
+ In order to use bootstrap methods effectively, how you resample the data, which is called bootstrap data-generating process, should contain the features of the underlying population data-generating process such as heterogeneity and clustering. 

+ Although we never know about the true data-generating process, we need to guess it somehow and use an appropriate bootstrap DGP. This is the first thing. 

+ The second point is the asymptotic refinement of bootstrap. 
+ By the way, is it okay about the meaning of asymptotic pivotal statistics?


---
class: middle

## Various bootstrap DGPs

1.<b><span style="color:blue">Paired (or nonparametric) bootstrap</span></b>
  * **Idea**: resample the data with size $n$

<br>

2.<b><span style="color:blue">Wild bootstrap</span></b>
  * **Idea**: resample the residuals with size $n$
  * can handle heteroskedasticity better than the paired bootstrap
  * For hypothesis testing, use the <b><span style="color:red">restricted wild bootstrap</span></b>

<br>

3.<b><span style="color:blue">Wild cluster bootstrap</span></b>
  * **Idea**: resample the $G$ cluster-level residuals
  * can handle both heteroskedasticity and within-cluster error correlation
  * For hypothesis testing, use the <b><span style="color:red">restricted wild cluster bootstrap</span> </b>([Cameron, Gelbach, and Miller (2008)](https://direct.mit.edu/rest/article-abstract/90/3/414/57731/Bootstrap-Based-Improvements-for-Inference-with?redirectedFrom=fulltext))


???
+ https://core.ac.uk/download/pdf/6494253.pdf
+ So, bootstrap is the popular and powerful tool for inference but the key assumption is that we need to bootstrap data-generating process recreate features of the underlying true data-generating process for the data. 
+ Because of this, we have multiple bootstrap methods which assume different data-generating process. 

+ first one is the paired bootstrap method. The process is pretty simple. Suppose that the size of the data is $n$. Then we resample $n$ observations allowing for duplicates. 

+ The wild bootstrap should be used for estimating standard errors or forming confidence intervals. But for hypothesis testing, we should use restricted wild bootstrap. Actually, one advantage of the wild bootstrap is that we can easily impose the null hypothesis on the bootstrap data-generating process. 

+ In the presence of clustered errors, we can use wild cluster bootstrap, and Cameron, Gelbach, and Miller (2008) recommend to use he restricted wild cluster bootstrap for hypothesis testing. 


---
class: middle 

## Wild bootstrap: R implementation

Use `boottest()` function from `fwildclusterboot` package
+ For further details see [[this](https://s3alfisc.github.io/fwildclusterboot/articles/fwildclusterboot.html)] 

.content-box-green[**Syntax:**]

```{r, eval=F}
boottest(
  regression result,
  B = the number of bootstrap replications,
  param = name of the model parameter to be tested,
  clustid = name of the clustering variables 
)
```

But, `boottest()` is currently not supported for hypotheses about more than one parameter.

---
class: middle
name: ex
## Exercise 4: 2022 Final Test Problem 4 (modified)
.small[
Suppose $Y=\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3 + e$.

We studied Wald tests about multiple regression coefficients. Suppose $H_0: \, \beta_1=\beta_2=\beta_3=0$.

(a) After running the regression, what are the steps involved in calculating the Wald statistic? You can write down a version of the Wald statistic and describe how you get the various parts.)

<span style="color:blue">Hint:</span>
+ Wald statistic: $W =(\hat{\theta}-\theta)^{\prime}\hat{\mathbf{V}}^{-1}_{\hat{\theta}}(\hat{\theta}-\theta)$
  * What are $\hat{\theta}$ and $\theta$ in this case?
  * How to obtain $\hat{V}_{\hat{\theta}}$? Note that we know the estimated covariance matrix of $\hat{\beta}$ (i.e. $\hat{V}_{\hat{\beta}}$) from the regression result. 


(b) Now that you have your Wald statistic, what should you do with it to determine whether to reject the null hypothesis? Consider two cases: a regression where clustering is not an issue and one where there are 40 clusters. 
]

---


class: inverse, center, middle
name: code

# Code review for assignment 7

<html><div style='float:left'></div><hr color='#EB811B' size=1px width=796px></html>

<!-- https://cran.r-project.org/web/packages/foreach/vignettes/foreach.html -->
---
class: middle

## `foreach()` from `foreach` package

`foreach()` function is another way of doing loop.

.content-box-green[**Basic syntax**]

```{r, eval=F}
foreach(iterating variables)%do%{
  iterating operations
}
```

<br>

.content-box-green[**NOTE 1**:]

The syntax of `for each` function looks alike `for loop` function, but it's different in a couple of details. 

+ Iterating variables are specified like `i=1:5` not `i in 1:5`
+ You need `%do%`

---
class: middle

Let's see the difference between `foreach` and `for loop`.

.content-box-green[**Example**] Compute $\sqrt{x}$ for $x=\{1,2,3\}$.

.left5[
.content-box-blue[`foreach` way]
.medium-code[
```{r}
library(foreach)

result <- 
  foreach(i=1:3, .combine="c") %do%{
    sqrt(i)
  }

result
```
  ]
]

.right5[
.content-box-blue[`for loop` way]
.medium-code[
```{r}
# You need an empty object to store results!!
result <- c() 

for(i in 1:3){
    result[i] <- sqrt(i)
  }

result
```
  ]
]

.content-box-green[**NOTE 2**:]

`foreach()` returns the results in a list (by default).
  * how the list of results is combined into a single object can be specified with  `.combine`.
    - e.g. `.combine="c"`
  * So, unlike `for loop`, you don't need to create a storage object! 

???
+ `foreach()` is different from `for loop` function in that `foreach()` returns 

---
## Parallel computing using `foreach` package

Another difference from `for loop` is that `foreach()` can do parallel processing by its own . 

<!-- + https://ethen8181.github.io/Business-Analytics/R/efficient_looping/efficient_looping.html -->

.content-box-green[**step1:Tell R to do parallel processing**]
```{r, eval=F}
# === Preparation === #
library(foreach)
library(future)
library(doFuture)
# Tells future package to use multiple cores on your computer.
plan(multisession, workers=availableCores()-1) 

# For reproducibility, use random number generator for parallel processing
library(doRNG)
registerDoFuture()
registerDoRNG(seed=580724) 
```

.content-box-green[**step2: Run `foreach()` function using `%dopar%`**]
For example,
```{r, eval=F}
results <- foreach(b = 1:B, .combine=rbind) %dopar% {
  ...
}
```

???
+ That being said, it is unlikely that you need to write codes for parallel processing in your test. So don't worry about it. 







